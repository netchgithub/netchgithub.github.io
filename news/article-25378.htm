<!doctype html>
<html class="no-js" lang="zh-CN">

<head>
        <link rel="canonical" href="https://netchgithub.github.io/news/article-25378.htm" />
    <meta charset="utf-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">
    <title>深度学习自定义自动求导函数</title>
        <meta name="description" content="参考：链接1，链接2 官方示例： import torch from torch.autograd import Variable   class MyReLU(torch.autograd.Func" />
        <link rel="icon" href="/assets/website/img/netchgithub/favicon.ico" type="image/x-icon"/>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="manifest" href="site.webmanifest">
    <!-- CSS here -->
    <link rel="stylesheet" href="/assets/website/css/netchgithub/bootstrap.min.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/owl.carousel.min.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/flaticon.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/slicknav.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/animate.min.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/magnific-popup.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/fontawesome-all.min.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/themify-icons.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/slick.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/nice-select.css">
    <link rel="stylesheet" href="/assets/website/css/netchgithub/style.css">
    
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-WPZ6L4HTT1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-WPZ6L4HTT1');
</script>
    <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-3332997411212854"
     crossorigin="anonymous"></script>
</head>

<body data-page="detail">
        <!-- Preloader Start -->
    <div id="preloader-active">
        <div class="preloader d-flex align-items-center justify-content-center">
            <div class="preloader-inner position-relative">
                <div class="preloader-circle"></div>
                <div class="preloader-img pere-text">
                    <img src="/assets/website/img/netchgithub/logo/logo.png" alt="">
                </div>
            </div>
        </div>
    </div>
    <!-- Preloader Start -->
    <header>
        <!-- Header Start -->
        <div class="header-area header-transparrent ">
            <div class="main-header header-sticky">
                <div class="container">
                    <div class="row align-items-center">
                        <!-- Logo -->
                        <div class="col-md-4">
                            <div class="logo">
                                                                <a href="/">
                                    <span>Netch Github</span>
                                </a>
                                                            </div>
                        </div>
                        <div class="col-xl-8 col-lg-8 col-md-8">
                            <!-- Main-menu -->
                            <div class="main-menu f-right d-none d-lg-block">
                                <nav>
                                    <ul id="navigation">
                                                                                <li><a href="/"> 首页</a></li>
                                                                                <li><a href="/free-nodes/"> 免费节点</a></li>
                                                                                <li><a href="/paid-subscribe/"> 推荐机场</a></li>
                                                                                <li><a href="/news/"> 新闻资讯</a></li>
                                                                                <li><a href="#">关于</a></li>
                                        <li><a href="#">联系</a></li>
                                    </ul>
                                </nav>
                            </div>
                        </div>
                        <!-- Mobile Menu -->
                        <div class="col-12">
                            <div class="mobile_menu d-block d-lg-none"></div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Header End -->
    </header>
    <main>
        <!-- Slider Area Start-->
        <div class="services-area">
            <div class="container">
                <!-- Section-tittle -->
                <div class="row d-flex justify-content-center">
                    <div class="col-lg-8">
                        <div class="section-tittle text-center mb-80">
                            <h1>深度学习自定义自动求导函数</h1>
                            <p>
                                <a href="/">首页</a> / <a href="/news/">新闻资讯</a> / 正文
                            </p>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Slider Area End-->
        <!-- We Create Start -->
        <div class="we-create-area create-padding">
            <div class="container">
                <div class="row">
                    <div class="col-md-9">
                                          				  				  				<div id="content_views" class="markdown_views prism-atom-one-dark"> <p>参考：<a href="http://www.m6000.cn/wp-content/themes/begin%20lts/inc/go.php?url=https://blog.csdn.net/Hungryof/article/details/78346304"  rel="nofollow">链接1</a>，<a href="http://www.m6000.cn/wp-content/themes/begin%20lts/inc/go.php?url=https://blog.csdn.net/tsq292978891/article/details/79364140"  rel="nofollow">链接2</a></p> <p>官方示例：</p> <pre><code class="prism language-python"><span class="token keyword">import</span> torch <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Variable   <span class="token keyword">class</span> <span class="token class-name">MyReLU</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">"""     We can implement our own custom autograd Functions by subclassing     torch.autograd.Function and implementing the forward and backward passes     which operate on Tensors.     """</span>      @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token triple-quoted-string string">"""         In the forward pass we receive a Tensor containing the input and return         a Tensor containing the output. ctx is a context object that can be used         to stash information for backward computation. You can cache arbitrary         objects for use in the backward pass using the ctx.save_for_backward method.          """</span>         ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span> <span class="token comment"># ctx 用来保存反向求导所需要的数据,也就是可以在backward（）函数中使用的变量。</span>         <span class="token keyword">return</span> <span class="token builtin">input</span><span class="token punctuation">.</span>clamp<span class="token punctuation">(</span><span class="token builtin">min</span><span class="token operator">=</span><span class="token number">0</span><span class="token punctuation">)</span>      @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token triple-quoted-string string">"""         In the backward pass we receive a Tensor containing the gradient of the loss         with respect to the output, and we need to compute the gradient of the loss         with respect to the input.         """</span>         <span class="token builtin">input</span><span class="token punctuation">,</span> <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors         grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>clone<span class="token punctuation">(</span><span class="token punctuation">)</span>         grad_input<span class="token punctuation">[</span><span class="token builtin">input</span> <span class="token operator">&lt;</span> <span class="token number">0</span><span class="token punctuation">]</span> <span class="token operator">=</span> <span class="token number">0</span>         <span class="token keyword">return</span> grad_input <span class="token comment">#反向传播求梯度，如果该参数为网络需要更新的参数，那么该梯度会被保存，方便之后的参数更新或者优化。</span>   dtype <span class="token operator">=</span> torch<span class="token punctuation">.</span>FloatTensor <span class="token comment"># dtype = torch.cuda.FloatTensor # Uncomment this to run on GPU</span>  <span class="token comment"># N is batch size; D_in is input dimension;</span> <span class="token comment"># H is hidden dimension; D_out is output dimension.</span> N<span class="token punctuation">,</span> D_in<span class="token punctuation">,</span> H<span class="token punctuation">,</span> D_out <span class="token operator">=</span> <span class="token number">64</span><span class="token punctuation">,</span> <span class="token number">1000</span><span class="token punctuation">,</span> <span class="token number">100</span><span class="token punctuation">,</span> <span class="token number">10</span>  <span class="token comment"># Create random Tensors to hold input and outputs, and wrap them in Variables.</span> x <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_in<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span> y <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>N<span class="token punctuation">,</span> D_out<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">False</span><span class="token punctuation">)</span>  <span class="token comment"># Create random Tensors for weights, and wrap them in Variables.</span> w1 <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>D_in<span class="token punctuation">,</span> H<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span> w2 <span class="token operator">=</span> Variable<span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>H<span class="token punctuation">,</span> D_out<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">type</span><span class="token punctuation">(</span>dtype<span class="token punctuation">)</span><span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>  learning_rate <span class="token operator">=</span> <span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span> <span class="token keyword">for</span> t <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span><span class="token number">500</span><span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token comment"># To apply our Function, we use Function.apply method. We alias this as 'relu'.</span>     relu <span class="token operator">=</span> MyReLU<span class="token punctuation">.</span><span class="token builtin">apply</span>      <span class="token comment"># Forward pass: compute predicted y using operations on Variables; we compute</span>     <span class="token comment"># ReLU using our custom autograd operation.</span>     y_pred <span class="token operator">=</span> relu<span class="token punctuation">(</span>x<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w1<span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>w2<span class="token punctuation">)</span>      <span class="token comment"># Compute and print loss</span>     loss <span class="token operator">=</span> <span class="token punctuation">(</span>y_pred <span class="token operator">-</span> y<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">pow</span><span class="token punctuation">(</span><span class="token number">2</span><span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token punctuation">)</span>     <span class="token keyword">print</span><span class="token punctuation">(</span>t<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>item<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>      <span class="token comment"># Use autograd to compute the backward pass.</span>     loss<span class="token punctuation">.</span>backward<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token comment"># 反向传播会将可训练参数梯度保存。</span>      <span class="token comment"># Update weights using gradient descent</span>     w1<span class="token punctuation">.</span>data <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w1<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data     w2<span class="token punctuation">.</span>data <span class="token operator">-=</span> learning_rate <span class="token operator">*</span> w2<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data      <span class="token comment"># Manually zero the gradients after updating weights</span>     w1<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token comment">#梯度清零。</span>     w2<span class="token punctuation">.</span>grad<span class="token punctuation">.</span>data<span class="token punctuation">.</span>zero_<span class="token punctuation">(</span><span class="token punctuation">)</span>  </code></pre> <p>类继承：<br /><code>torch.autograd.Function</code>，只需要定义正向传播和反向传播函数，正向传播就是自己定义函数的计算方法；反向传播则是求导梯度，<code>ctx</code>这个东西就当做<code>self</code>来对待就行，可以用来存储反向求导要求的数据，比如正向传播的结果或者输入。</p> <h3> <a id="Linear__83" rel="nofollow"></a>自定义Linear 操作：</h3> <pre><code class="prism language-python"><span class="token keyword">import</span> torch <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> Function <span class="token keyword">import</span> warnings warnings<span class="token punctuation">.</span>filterwarnings<span class="token punctuation">(</span><span class="token string">"ignore"</span><span class="token punctuation">)</span>    <span class="token keyword">class</span> <span class="token class-name">LinearFunction1</span><span class="token punctuation">(</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token triple-quoted-string string">""" 描述：在pytorch中自定义一个操作，并定义它的梯度求法"""</span>     @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>   <span class="token comment"># shape: n,m,  m nout</span>         <span class="token comment"># ctx.needs_input_grad = (False,True,True)</span>         output <span class="token operator">=</span> torch<span class="token punctuation">.</span>mm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">)</span>  <span class="token comment"># n,m; m,c_out</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>             output <span class="token operator">+=</span> bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>output<span class="token punctuation">)</span>             <span class="token comment"># output += torch.unsqueeze(bias,dim=0).expand_as(output)</span>             <span class="token comment"># output += bias   #广播。</span>         <span class="token comment"># ctx.save_for_backward(output)</span>         <span class="token keyword">return</span> output      @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_outputs<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors         grad_input <span class="token operator">=</span> <span class="token boolean">None</span>         grad_weight <span class="token operator">=</span> <span class="token boolean">None</span>         grad_bias <span class="token operator">=</span> <span class="token boolean">None</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_input <span class="token operator">=</span> grad_outputs @ <span class="token punctuation">(</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>   <span class="token comment"># n,c_out;c_out,m</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_weight <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span> @ grad_outputs  <span class="token comment"># m,n    n,c_out</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span> <span class="token operator">and</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>              grad_bias <span class="token operator">=</span> grad_outputs<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>          <span class="token keyword">return</span> grad_input<span class="token punctuation">,</span>grad_weight<span class="token punctuation">,</span>grad_bias  <span class="token comment"># Inherit from Function</span> <span class="token keyword">class</span> <span class="token class-name">LinearFunction</span><span class="token punctuation">(</span>Function<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token comment"># Note that both forward and backward are @staticmethods</span>     @<span class="token builtin">staticmethod</span>     <span class="token comment"># bias is an optional argument</span>     <span class="token keyword">def</span> <span class="token function">forward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token operator">=</span><span class="token boolean">None</span><span class="token punctuation">)</span><span class="token punctuation">:</span>         ctx<span class="token punctuation">.</span>save_for_backward<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias<span class="token punctuation">)</span>         output <span class="token operator">=</span> <span class="token builtin">input</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span>weight<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>  <span class="token comment"># 20,20; 30,20 -&gt; 20,30</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span><span class="token punctuation">:</span>             output <span class="token operator">+=</span> bias<span class="token punctuation">.</span>unsqueeze<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span><span class="token punctuation">.</span>expand_as<span class="token punctuation">(</span>output<span class="token punctuation">)</span>         <span class="token keyword">return</span> output      <span class="token comment"># This function has only a single output, so it gets only one gradient</span>     @<span class="token builtin">staticmethod</span>     <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>         <span class="token comment"># This is a pattern that is very convenient - at the top of backward</span>         <span class="token comment"># unpack saved_tensors and initialize all gradients w.r.t. inputs to</span>         <span class="token comment"># None. Thanks to the fact that additional trailing Nones are</span>         <span class="token comment"># ignored, the return statement is simple even when the function has</span>         <span class="token comment"># optional inputs.</span>         <span class="token builtin">input</span><span class="token punctuation">,</span> weight<span class="token punctuation">,</span> bias <span class="token operator">=</span> ctx<span class="token punctuation">.</span>saved_tensors         grad_input <span class="token operator">=</span> grad_weight <span class="token operator">=</span> grad_bias <span class="token operator">=</span> <span class="token boolean">None</span>          <span class="token comment"># These needs_input_grad checks are optional and there only to</span>         <span class="token comment"># improve efficiency. If you want to make your code simpler, you can</span>         <span class="token comment"># skip them. Returning gradients for inputs that don't require it is</span>         <span class="token comment"># not an error.</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_input <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>mm<span class="token punctuation">(</span>weight<span class="token punctuation">)</span>   <span class="token comment"># 20 30 , 30 20  -&gt; 20 20   或者 20 30 30 20</span>         <span class="token keyword">if</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_weight <span class="token operator">=</span> grad_output<span class="token punctuation">.</span>t<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>mm<span class="token punctuation">(</span><span class="token builtin">input</span><span class="token punctuation">)</span>  <span class="token comment"># 30 20, 20 20 - &gt; 30 20</span>         <span class="token keyword">if</span> bias <span class="token keyword">is</span> <span class="token operator">not</span> <span class="token boolean">None</span> <span class="token operator">and</span> ctx<span class="token punctuation">.</span>needs_input_grad<span class="token punctuation">[</span><span class="token number">2</span><span class="token punctuation">]</span><span class="token punctuation">:</span>             grad_bias <span class="token operator">=</span> grad_output<span class="token punctuation">.</span><span class="token builtin">sum</span><span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">)</span>          <span class="token keyword">return</span> grad_input<span class="token punctuation">,</span> grad_weight<span class="token punctuation">,</span> grad_bias </code></pre> <p>也就是定义正向传播和反向传播并保存需要的数据到context中即可，函数内的数学运算可以不是pytorch支持的运算而只需要是python支持支持的即可（个人理解是这样）。</p> <p>测试操作是否正确：</p> <pre><code class="prism language-python"><span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd <span class="token keyword">import</span> gradcheck  linear <span class="token operator">=</span> LinearFunction<span class="token punctuation">.</span><span class="token builtin">apply</span>   <span class="token comment">#这里使用上边的为什么不行，去个别名。</span> <span class="token builtin">input</span> <span class="token operator">=</span> <span class="token punctuation">(</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span>size<span class="token operator">=</span><span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">)</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span> requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">30</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> test <span class="token operator">=</span> gradcheck<span class="token punctuation">(</span>linear<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>test<span class="token punctuation">)</span>   linear <span class="token operator">=</span> LinearFunction1<span class="token punctuation">.</span><span class="token builtin">apply</span>   <span class="token comment">#这里使用上边的为什么不行，去个别名。</span> <span class="token builtin">input</span> <span class="token operator">=</span> <span class="token punctuation">(</span>torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">20</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">,</span> torch<span class="token punctuation">.</span>randn<span class="token punctuation">(</span><span class="token number">20</span><span class="token punctuation">,</span><span class="token number">30</span><span class="token punctuation">,</span>dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>double<span class="token punctuation">,</span>requires_grad<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span><span class="token punctuation">)</span> test <span class="token operator">=</span> gradcheck<span class="token punctuation">(</span>linear<span class="token punctuation">,</span> <span class="token builtin">input</span><span class="token punctuation">,</span> eps<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">6</span><span class="token punctuation">,</span> atol<span class="token operator">=</span><span class="token number">1e</span><span class="token operator">-</span><span class="token number">4</span><span class="token punctuation">)</span> <span class="token keyword">print</span><span class="token punctuation">(</span>test<span class="token punctuation">)</span> </code></pre> <p>可以看到两种方都返回的是<code>True</code>,需要注意的是自己定义的操作输入的形状等问题而已。</p> <h4> <a id="_179" rel="nofollow"></a>用自己的操作来建立模型：</h4> <pre><code class="prism language-cpp">import torch<span class="token punctuation">.</span>nn as nn <span class="token keyword">class</span> <span class="token class-name">Linear</span><span class="token punctuation">(</span>nn<span class="token punctuation">.</span>Module<span class="token punctuation">)</span><span class="token operator">:</span>     def <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> input_features<span class="token punctuation">,</span> output_features<span class="token punctuation">,</span> bias<span class="token operator">=</span>True<span class="token punctuation">)</span><span class="token operator">:</span>         <span class="token function">super</span><span class="token punctuation">(</span>Linear<span class="token punctuation">,</span>self<span class="token punctuation">)</span><span class="token punctuation">.</span><span class="token function">__init__</span><span class="token punctuation">(</span><span class="token punctuation">)</span>          self<span class="token punctuation">.</span>input_features <span class="token operator">=</span> input_features         self<span class="token punctuation">.</span>output_features <span class="token operator">=</span> output_features          self<span class="token punctuation">.</span>weight <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Parameter</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token function">randn</span><span class="token punctuation">(</span>input_features<span class="token punctuation">,</span> output_features<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token keyword">if</span> bias<span class="token operator">:</span>             self<span class="token punctuation">.</span>bias <span class="token operator">=</span> nn<span class="token punctuation">.</span><span class="token function">Parameter</span><span class="token punctuation">(</span>torch<span class="token punctuation">.</span><span class="token function">randn</span><span class="token punctuation">(</span>output_features<span class="token punctuation">)</span><span class="token punctuation">)</span>         <span class="token keyword">else</span><span class="token operator">:</span>             self<span class="token punctuation">.</span><span class="token function">register_parameter</span><span class="token punctuation">(</span><span class="token string">"bias"</span><span class="token punctuation">,</span> None<span class="token punctuation">)</span>          <span class="token macro property"># self.weight.uniform(-0.1, 0.1)</span>         nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span><span class="token function">kaiming_uniform</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>weight<span class="token punctuation">)</span>         <span class="token keyword">if</span> bias<span class="token operator">:</span>             nn<span class="token punctuation">.</span>init<span class="token punctuation">.</span><span class="token function">kaiming_uniform</span><span class="token punctuation">(</span>self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span>      def <span class="token function">forward</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span>x<span class="token punctuation">)</span><span class="token operator">:</span>         <span class="token keyword">return</span> <span class="token function">LinearFunction1</span><span class="token punctuation">(</span>x<span class="token punctuation">,</span> self<span class="token punctuation">.</span>weight<span class="token punctuation">,</span> self<span class="token punctuation">.</span>bias<span class="token punctuation">)</span> # 调用自定义的操作。   </code></pre> <p>在自定义操作的基础上建立的<code>layer</code>就与其他<code>layer</code>一样都可以自动求导和优化参数了。</p> <h4> <a id="_207" rel="nofollow"></a>不可导情况：</h4> <p>上边的线性变换是的运算是可导的情况，也就是可以从输出一步步的用导数或者运算来表达，如果遇到那种不可导的情况，也就是无法显式的表达导数该怎么办？那就是自己制定导数求法，比如近似求导或者干脆用另一个黑盒函数来进行代替求导。那么既然是黑盒函数，那么反向的传播的时候函数中间的值的梯度什么的就很难进行计算了，这种问题就需要对<code>backward</code>函数进行稍微的改变：</p> <pre><code class="prism language-python"><span class="token triple-quoted-string string">"""当某个操作是不可导的，但是你却用了近似的方法来代替。"""</span> <span class="token keyword">from</span> torch<span class="token punctuation">.</span>autograd<span class="token punctuation">.</span>function <span class="token keyword">import</span> once_differentiable  <span class="token keyword">def</span> <span class="token function">un_differentibale_function</span><span class="token punctuation">(</span>grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>      <span class="token string">"一些列不可导的神奇操作"</span>      grad_output_changed <span class="token operator">=</span> <span class="token boolean">None</span>      <span class="token keyword">return</span>  grad_output_changed   @<span class="token builtin">staticmethod</span> @once_differentiable <span class="token keyword">def</span> <span class="token function">backward</span><span class="token punctuation">(</span>ctx<span class="token punctuation">,</span> grad_output<span class="token punctuation">)</span><span class="token punctuation">:</span>     <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token builtin">type</span><span class="token punctuation">(</span>grad_output<span class="token punctuation">)</span><span class="token punctuation">)</span>     grad_output_changed <span class="token operator">=</span> un_differentibale_function<span class="token punctuation">(</span>grad_output<span class="token punctuation">)</span>     grad_input <span class="token operator">=</span> grad_output_changed     <span class="token keyword">return</span> grad_input </code></pre> <p><code>@once_differentiable</code> 神马意思，我也说不清，就当做隐式求导或者近似求导吧。</p> </p></div> 			                <div class="clearfix"></div>
                <div class="col-md-12 mt-5">
                                        <p>上一个：<a href="/news/article-24918.htm">宠物粮食店一年利润怎么样计算出来多少 宠物粮食店一年利润怎么样计算出来多少钱</a></p>
                                        <p>下一个：<a href="/news/article-25379.htm">SpringMVC拦截器配置方式</a></p>
                                    </div>
                                    </div>
                    <div class="col-md-3">
                        <div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">热门文章</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2"><a href="/news/article-25379.htm" title="SpringMVC拦截器配置方式">SpringMVC拦截器配置方式</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-9-17-node-share.htm" title="「9月17日」最高速度21M/S，2024年Netch Github每天更新免费机场订阅节点链接">「9月17日」最高速度21M/S，2024年Netch Github每天更新免费机场订阅节点链接</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-10-25-netch-node.htm" title="「10月25日」最高速度22.9M/S，2024年Netch Github每天更新免费机场订阅节点链接">「10月25日」最高速度22.9M/S，2024年Netch Github每天更新免费机场订阅节点链接</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-10-13-node-share-links.htm" title="「10月13日」最高速度19.4M/S，2024年Netch Github每天更新免费机场订阅节点链接">「10月13日」最高速度19.4M/S，2024年Netch Github每天更新免费机场订阅节点链接</a></li>
                        <li class="py-2"><a href="/news/article-20777.htm" title="猫进屋十大预兆黑色猫（猫进屋好不好?）">猫进屋十大预兆黑色猫（猫进屋好不好?）</a></li>
                        <li class="py-2"><a href="/news/article-18956.htm" title="开一家宠物食品店怎么样 开一家宠物食品店怎么样呢">开一家宠物食品店怎么样 开一家宠物食品店怎么样呢</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-10-20-node-share-links.htm" title="「10月20日」最高速度20.7M/S，2024年Netch Github每天更新免费机场订阅节点链接">「10月20日」最高速度20.7M/S，2024年Netch Github每天更新免费机场订阅节点链接</a></li>
                        <li class="py-2"><a href="/news/article-25841.htm" title="一文详解微服务架构">一文详解微服务架构</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-9-2-free-subscribe-node.htm" title="「9月2日」最高速度22.2M/S，2024年Netch Github每天更新免费机场订阅节点链接">「9月2日」最高速度22.2M/S，2024年Netch Github每天更新免费机场订阅节点链接</a></li>
                        <li class="py-2"><a href="/free-nodes/2024-11-3-netch-node.htm" title="「11月3日」最高速度18.8M/S，2024年Netch Github每天更新免费机场订阅节点链接">「11月3日」最高速度18.8M/S，2024年Netch Github每天更新免费机场订阅节点链接</a></li>
                    </ul>
    </div>
</div>

<div class="panel panel-default">
    <div class="panel-heading">
        <h3 class="panel-title">归纳</h3>
    </div>
    <div class="panel-body">
        <ul class="p-0 x-0" style="list-style: none;margin: 0;padding: 0;">
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">42</span> <a href="/date/2024-12/" title="2024-12 归档">2024-12</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">34</span> <a href="/date/2024-11/" title="2024-11 归档">2024-11</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">31</span> <a href="/date/2024-10/" title="2024-10 归档">2024-10</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">30</span> <a href="/date/2024-09/" title="2024-09 归档">2024-09</a></h4>
            </li>
                        <li class="py-2">
                <h4><span class="badge" style="float: right;">13</span> <a href="/date/2024-08/" title="2024-08 归档">2024-08</a></h4>
            </li>
                    </ul>
    </div>
</div>

                    </div>
                </div>
            </div>
        </div>
        <!-- We Create End -->
    </main>
        <footer>
        <!-- Footer Start-->
        <div class="footer-main" data-background="assets/img/shape/footer_bg.png">
            <!-- footer-bottom aera -->
            <div class="footer-bottom-area footer-bg">
                <div class="container">
                    <div class="footer-border">
                        <div class="row d-flex align-items-center">
                            <div class="col-xl-12 ">
                                <div class="footer-copy-right text-center">
                                    <p>
                                        Netch Github节点订阅官网 版权所有
                                        <br />
                                        Powered by WordPress
                                    </p>
                                </div>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        <!-- Footer End-->
    </footer>
    <!-- JS here -->
    <!-- All JS Custom Plugins Link Here here -->
    <script src="/assets/website/js/frontend/netchgithub/vendor/modernizr-3.5.0.min.js"></script>
    <!-- Jquery, Popper, Bootstrap -->
    <script src="/assets/website/js/frontend/netchgithub/vendor/jquery-3.5.1.min.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/popper.min.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/bootstrap.min.js"></script>
    <!-- Jquery Mobile Menu -->
    <script src="/assets/website/js/frontend/netchgithub/jquery.slicknav.min.js"></script>
    <!-- Jquery Slick , Owl-Carousel Plugins -->
    <script src="/assets/website/js/frontend/netchgithub/owl.carousel.min.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/slick.min.js"></script>
    <!-- Date Picker -->
    <script src="/assets/website/js/frontend/netchgithub/gijgo.min.js"></script>
    <!-- One Page, Animated-HeadLin -->
    <script src="/assets/website/js/frontend/netchgithub/wow.min.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/animated.headline.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/jquery.magnific-popup.js"></script>
    <!-- Scrollup, nice-select, sticky -->
    <script src="/assets/website/js/frontend/netchgithub/jquery.scrollUp.min.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/jquery.nice-select.min.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/jquery.sticky.js"></script>
    <!-- contact js -->
    <script src="/assets/website/js/frontend/netchgithub/contact.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/jquery.form.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/jquery.validate.min.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/mail-script.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/jquery.ajaxchimp.min.js"></script>
    <!-- Jquery Plugins, main Jquery -->
    <script src="/assets/website/js/frontend/netchgithub/plugins.js"></script>
    <script src="/assets/website/js/frontend/netchgithub/main.js"></script>
    <script src="https://www.freeclashnode.com/assets/js/frontend/invite-url.js"></script><script src="/assets/website/js/frontend/G.js"></script>
</body>

</html>